# [UM-042] Web scraping

## What is web scraping?

**Web scraping** is concerned with extracting data from websites, in particular data that would be difficult to get on a large scale using traditional data collection methods. There is a whole industry built around web scraping, as it is used to track product price changes or discounts, to gather data from social profiles, to capture real estate listings, in search engine optimization (SEO), etc.

Scraping a web page involves downloading the page and extracting data from it. Both things can be done in many ways, in particular with Python tools. There are also commercial web scraping applications, such as **Apify** and **Octoparse**. This course uses the Python packages **Requests** and **Beautiful Soup**.

## HTML and the browser

Suppose that your browser (let us assume that you use Google Chrome) is displaying a web page on the screen. Right-clicking anywhere on the page opens a contextual menu. Then, selecting the *View Page Source* option, a new tab will open, displaying a HTML document. In the simplest case, which is the one covered in this lecture, this HTML document corresponds to the page that the browser was displaying. 

But not all pages are that simple. Some use a technology called **AJAX** (Asynchronous JavaScript And XML) in two-step process as follows:

1. The page corresponding to the URL that you enter is loaded.

2. A JavaScript program creates a `XMLHttpRequest` object.

3. The `XMLHttpRequest` object sends a request to a web server.

4. The server sends a new HTML document back to the browser, which the browser displays on the screen. This second document corresponds to the page that you are actually seeing.

The tools provided by the Python package **Requests** can only capture the first page, which is not always the one from which you wish to scrape the information. To get the second one, web scrapers use a tool called **Selenium**, whose discussion we postpone to the next lecture.

Also in the contextual menu of the browser, the *Inspect* option can help you to identify an element of a web page. Right-clicking on a part of the page currently displayed by the browser and selecting this option, the screen is split, leaving the web page on one side and displaying on the other side the *Developer Tools* panel, which provides many choices: *Elements*, *Console*, *Network*, etc. The first one contains the page's DOM tree and gives you full access to the source code of the page currently displayed, which may be different from the one you called, as explained above. The element of the page on which you have clicked appears highlighted.

## The package Requests

In Python, files can be downloaded from Internet sources in multiple ways. Old tutorials suggest using the package `urllib`, which is part of the Python Standard Library. Nowadays, Requests, included in the Anaconda distribution, is the favorite choice of the practitioners.

Let us refresh the context. Through the browser, you can access to resources, specifying a **Uniform Resource Locator** (URL). At the beginning of the URL, we find the the protocol used to access the resource, followed by a colon and two forward slashes. This is usually HTTPS, a secure version of HTTP. 

The **Hypertext Transfer Protocol** (HTTP) was designed to enable communications between clients and servers. For instance, a client (such as your browser) sends a **HTTP request** to the server. Then, the server returns the response to the client. The response contains status information about the request and, if the request is accepted, the requested content.

**GET** is one of the most common HTTP methods. It is used to request data from a specified resource. The Requests function `get()` is a Python implementation. You can manage this as follows:

```
import requests
html_str = requests.get(url).text
```

`get()` returns a `requests` object (type `requests.models.Response`), containing data about the request. The attribute `text` of this object is a string which, for an ordinary web page, is the HTML source code. Now you can parse this string with the function `BeautifulSoup()` from the package `bs4`, and then extract the information sought by means of the methods `.find()` and `.find_all()`. This information, after cleaning, can be exported to your preferred data format. In the example of this lecture, we export the data extracted to a CSV file.
