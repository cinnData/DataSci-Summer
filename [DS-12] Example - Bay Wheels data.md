# [DS-12] Example - Bay Wheels data

## Introduction

**Bike sharing systems** are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout an urban area. Using these systems, people are able to rent a bike from a one location and return it to a different place on an as-needed basis. According to PBSC Urban Solutions, there were, as of August 2021, nearly 10 million shared bikes and 3,000 bike sharing systems across the world. 

There is a great interest in these systems due to their important role in traffic, environmental and health issues. Apart from interesting real world applications of bike sharing systems, the characteristics of the data generated by these systems makes them attractive for researchers. Opposed to other transport services such as bus or subway, the duration of travel, departure location, arrival location and time elapsed is explicitly recorded. This feature turns bike sharing system into a virtual sensor network, which can be used for studying mobility within the area. Hence, it is expected that most of important events in the area could be detected via monitoring these data.

This example uses two years of data from the **Bay Wheels Bike Share Program**, which brings point-to-point bike sharing to Bay Area cities, serving Berkeley, Emeryville, Oakland, San Jose and San Francisco. Bay Wheels is a partnership between MTC, the five local governments, and Motivate (a subsidiary of Lyft).  

The Bay Wheels Program has undergone a bit of change. It started as the Bay Area Bike Share in August 2013. On June 28, 2017, the system was officially re-launched as Ford GoBike in a partnership with Ford Motor Company. After Motivate's acquisition by Lyft, the system was renamed to Bay Wheels on June 11, 2019. In 2018, Ford GoBike added **electric bicycles** and **dockless bike share**. Nowadays, electric bicycles account for two thirds of the rides. Classic bikes are docked, so they are picked at one station and left at another station (or at the same one). E-bikes can use the **docking stations** or can be locked to a **city bike rack**.

### The data set

The data for the example come in two tables. The table `bay_stations` (in a CSV file) contains data on 642 docking stations. These stations are not the same along the three-yer period, since the organization is dynamic. The columns are:

* `station_id`, a unique identifier of the station. The first two characters indicate the location, with values 'BK' (Berkeley), 'SF' (San Francisco), 'SJ' (San Jose) and 'OK' (Oakland).

* `station_name`, the name of the station, referred to its location. 

* `station_latitude`, the latitude of the station, with three decimals.

* `station_longitude`, the longitude of the station, with three decimals.

The table `bay_rides` (in five zipped CSV files) contains information on all rides starting in the years 2021, 2022 and 2023, a total of 7,162,392 rides. The columns are:

* `user_type`, either 'casual' or 'member'.

* `bike_type`, either 'classic' or 'electric'.

* `start_time`, when the bike was picked, as 'yyyy-mm-dd hh:mm:ss'. 

* `start_station_id`, the identifier of the docking station where the ride started, missing when no station was involved.

* `end_time`, when the bike was returned, as 'yyyy-mm-dd hh:mm:ss'. 

* `end_station_id`, the identifier of the docking station where the ride ended, missing when no station was involved.

## Questions

Q1. Add a column `hour` to the table `rides`, containing the hour of the start time, in `datetime64` format. Example: the hour for `2021-01-01 01:20:23` will be `2021-01-01 01:00:00`.

Q2. Group by `hour` and aggregate so you get a a new table with two columns, `casual` and `member` containing, for every hour, the total number of rides of the types of users. 

Q3. After aggregating the data in the preceding question, can see you see a **time trend** in the number of rides? To visualize the trend, would it be better to aggregate more, *e.g*. to use daily data? Do you see a similar trend for the two user types?

Q4. Can you describe in an easy way the pattern for **intraday variation** (across hours) of the number of rides? Is this pattern different for the two user types?

Q5. Same questions for **intraweek variation** (across weekdays).

Q6. What about **monthly seasonality**?

## Importing the data

We import Pandas as usual.

```
In [1]: import pandas as pd
```
The table `rides` has been split in three data sets stores in zipped CSV files in GitHub, with a common path. So we create a variable containing that path.

```
In [2]: path = 'https://raw.githubusercontent.com/mikecinnamon/DataSci/main/Data/'
```

Next, we import to data frames the five files.

```
In [3]: rides1 = pd.read_csv(path + 'bay_rides-1.csv.zip')
   ...: rides2 = pd.read_csv(path + 'bay_rides-2.csv.zip')
   ...: rides3 = pd.read_csv(path + 'bay_rides-3.csv.zip')
   ...: rides4 = pd.read_csv(path + 'bay_rides-4.csv.zip')
   ...: rides5 = pd.read_csv(path + 'bay_rides-5.csv.zip')
```

With the Pandas function `concat`, we can get the **union** of these three data sets.

```
In [4]: rides = pd.concat([rides1, rides2, rides3, rides4, rides5]))
```

## Exploring the data

We check the content of the data frame `rides` as in other examples, with the methods `.info()` and `.head()`. Everything is as expected. Note that the station ID is missing for some of the electric bikes, as explained in the introduction.

```
In [5]: rides.info()
<class 'pandas.core.frame.DataFrame'>
Index: 7162392 entries, 0 to 1162391
Data columns (total 6 columns):
 #   Column            Dtype 
---  ------            ----- 
 0   user_type         object
 1   bike_type         object
 2   start_time        object
 3   start_station_id  object
 4   end_time          object
 5   end_station_id    object
dtypes: object(6)
memory usage: 382.5+ MB
```
```
In [6]: rides.head()
Out[6]: 
  user_type bike_type           start_time start_station_id   
0    casual  electric  2021-01-01 00:01:01              NaN  \
1    member   classic  2021-01-01 00:01:11           SF-I24   
2    member  electric  2021-01-01 00:01:44              NaN   
3    casual  electric  2021-01-01 00:03:44         SF-G30-1   
4    casual  electric  2021-01-01 00:05:33              NaN   

              end_time end_station_id  
0  2021-01-01 00:31:31            NaN  
1  2021-01-01 00:21:46         SF-L29  
2  2021-01-01 01:01:05            NaN  
3  2021-01-01 00:44:51            NaN  
4  2021-01-01 00:22:02            NaN  
```

## Q1. Add a column with the hour

The start and end times have type `str`, so we can create the new column with string methods. There many ways to do it. A simple one is to drop the last six characters, which are the minutes and seconds, replacing them by the string `':00:00'`.

```
In [7]: rides['hour'] = rides['start_time'].str[:-6] + ':00:00'
   ...: rides.head()
Out[7]: 
  user_type bike_type           start_time start_station_id   
0    casual  electric  2021-01-01 00:01:01              NaN  \
1    member   classic  2021-01-01 00:01:11           SF-I24   
2    member  electric  2021-01-01 00:01:44              NaN   
3    casual  electric  2021-01-01 00:03:44         SF-G30-1   
4    casual  electric  2021-01-01 00:05:33              NaN   

              end_time end_station_id                 hour  
0  2021-01-01 00:31:31            NaN  2021-01-01 00:00:00  
1  2021-01-01 00:21:46         SF-L29  2021-01-01 00:00:00  
2  2021-01-01 01:01:05            NaN  2021-01-01 00:00:00  
3  2021-01-01 00:44:51            NaN  2021-01-01 00:00:00  
4  2021-01-01 00:22:02            NaN  2021-01-01 00:00:00  
```

We need to convert this new column to a datetime type, to be able to extract the weekdays. Type conversions in Pandas are easily managed with the method `.astype()`. 

```
In [8]: rides['hour'] = rides['hour'].astype('datetime64[ns]')
   ...: rides.info()
<class 'pandas.core.frame.DataFrame'>
Index: 7162392 entries, 0 to 1162391
Data columns (total 7 columns):
 #   Column            Dtype         
---  ------            -----         
 0   user_type         object        
 1   bike_type         object        
 2   start_time        object        
 3   start_station_id  object        
 4   end_time          object        
 5   end_station_id    object        
 6   hour              datetime64[ns]
dtypes: datetime64[ns](1), object(6)
memory usage: 437.2+ MB
```

## Q2. Aggregate to hourly data

To build the data set for this question we need two additional columns, `casual` and `member``. We create them as dummies, so that we can aggregate them to get the number of rides for each group.

```
In [9]: rides['casual'] = rides['user_type'] == 'casual'
   ...: rides['member'] = rides['user_type'] == 'member'
```

Now, we group by hour and aggregate with the function `sum()`. To get a cleaner picture, we include only the columns which are relevant for the rest of this example. Note the double bracketing, which is needed, since the columns included have to be specified as a list.

```
In [10]: df = rides[['hour', 'casual', 'member']].groupby(by='hour').sum()
    ...: df.head()
Out[10]: 
                     casual  member
hour                               
2021-01-01 00:00:00      39      21
2021-01-01 01:00:00      48      27
2021-01-01 02:00:00      41       9
2021-01-01 03:00:00      18       7
2021-01-01 04:00:00      12       4
```

Note that `hour` is no longer a column, but the index. This is the default of the method `.groupby()`. given the type conversions that we performed above, to data type `datetime64`, this index is a `DatetimeIndex`.

```
In [11]: df.index
Out[11]: 
DatetimeIndex(['2021-01-01 00:00:00', '2021-01-01 01:00:00',
               '2021-01-01 02:00:00', '2021-01-01 03:00:00',
               '2021-01-01 04:00:00', '2021-01-01 05:00:00',
               '2021-01-01 06:00:00', '2021-01-01 07:00:00',
               '2021-01-01 08:00:00', '2021-01-01 09:00:00',
               ...
               '2023-12-31 14:00:00', '2023-12-31 15:00:00',
               '2023-12-31 16:00:00', '2023-12-31 17:00:00',
               '2023-12-31 18:00:00', '2023-12-31 19:00:00',
               '2023-12-31 20:00:00', '2023-12-31 21:00:00',
               '2023-12-31 22:00:00', '2023-12-31 23:00:00'],
              dtype='datetime64[ns]', name='hour', length=26277, freq=None)
```

## Q3. Time trend

We get rid of the index name, so that it does not appear in the plots below, which could be confusing.

```
In [12]: df.index.name = None
```

For clarity, we create a new column with the total number of rides.

```
In [13]: df['total'] = df['member'] + df['casual']
```

Time trends are typically spotted by means of **line charts**. See below a sample using the hourly total number of rides.

```
In [14]: df['total'].plot(figsize=(8,5), title='Figure 1. Hourly total demand', color='black', linewidth=1);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-1.png)

We see here a combination of a trend with time-based patterns, but it is difficult to conclude much with so many observations and the current granularity of the data. Since intraday patterns can be responsible for a significant part of the variation that we see in the chart, we aggregate to a daily data set. We use the mean so the vertical scale in the successive charts remains the same. Note that here, we don't use `.groupby()`, but `.resample()`, and we don't need to create a new data set for plotting.

```
In [15]: df['total'].resample('D').mean().plot(figsize=(8,5), title='Figure 2. Daily total demand', color='black', linewidth=1);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-2.png)

Now, the picture is more clear, though part of the variation is probably due to weekends and holidays. Going a bit further, we can aggregate to a weekly data set, again with `.resample()`.

```
In [16]: df['total'].resample('W').mean().plot(figsize=(8,5), title='Figure 3. Weekly total demand', color='black', linewidth=1);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-3.png)

Even further, we can aggregate to a monthly data set. The chart shows a combination of a trend plus monthly seasonality.

```
In [17]: df['total'].resample('M').mean().plot(figsize=(8,5), title='Figure 4. Monthly total demand', color='black', linewidth=1);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-4.png)

To see whether the trend is the same for the two user types, we can draw separate charts, which show that the trend only happens in the member group. This may be due to workforce discountinuing remote work and going back to office after the Stay Home order.

```
In [18]: df['casual'].resample('M').mean().plot(figsize=(8,5), title="Figure 5. Casuals' monthly total demand", color='black', linewidth=1);

```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-5.png)

```
In [19]: df['member'].resample('M').mean().plot(figsize=(8,5), title="Figure 6. Members' monthly total demand", color='black', linewidth=1);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-6.png)

## Q4. Intraday variation

To examine the intraday variation, we need to extract an average value for every hour. First, we create a column containing (only) the hour, as an integer, with the method `.hour`.

```
In [20]: df['hour'] = df.index.hour
    ...: df.head()
Out[20]: 
                     casual  member  total  hour
2021-01-01 00:00:00      39      21     60     0
2021-01-01 01:00:00      48      27     75     1
2021-01-01 02:00:00      41       9     50     2
2021-01-01 03:00:00      18       7     25     3
2021-01-01 04:00:00      12       4     16     4
```

Nom, we group by hour and aggregate to 24 average values. For the casual users, this would be as follows.

```
In [21]: df[['casual', 'hour']].groupby('hour').mean().round(1)
Out[21]: 
      casual
hour        
0       34.3
1       26.3
2       19.9
3        8.5
4        8.2
5       12.3
6       28.4
7       70.5
8      126.5
9      123.3
10     130.6
11     158.2
12     184.6
13     191.4
14     201.0
15     217.8
16     242.3
17     269.9
18     228.5
19     155.1
20     101.9
21      90.6
22      79.1
23      54.7
```

You may prefere to see this in a **bar chart**. Comparing the charts of the two user types, we see the influence of work schedule in the intraday variation pattern of the members.  

```
In [22: df[['casual', 'hour']].groupby('hour').mean().plot.bar(figsize=(7,5),
    ...:    title="Figure 7. Intraday variation of casuals' average demand", color='gray', legend=False);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-7.png)

```
In [23]: df[['member', 'hour']].groupby('hour').mean().plot.bar(figsize=(7,5),
    ...:    title="Figure 8. Intraday variation of members' average demand", color='gray', legend=False);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-8.png)

You may prefer to pack both charts as a **stacked bar chart**, which is east in Pandas, as we seen next.

```
In [24]: df[['casual', 'member', 'hour']].groupby('hour').mean().plot.bar(figsize=(7,5),
    ...:    title='Figure 9. Intraday variation of average demand', color=['0.4', '0.7'], stacked=True);

```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-9.png)

## Q5. Intraweek variation

A similar approach can be followed for the intraweek variation. First, we create a new column extracting the weekday from the index, now with `.weekday` (Monday = 0, Sunday = 6).

```
In [25]: df['weekday'] = df.index.weekday
``` 

The stacked bar chart is obtained as in Q4. As there, the different patterns suggests that members and casuals use the bike  travelling with different purposes (work/leisure). 

```
In [26]: df[['casual', 'member', 'weekday']].groupby('weekday').mean().plot.bar(figsize=(7,5),
    ...:    title= 'Figure 10. Intraweek variation of total demand', color=['0.4', '0.7'], stacked=True);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-10.png)

## Q6. Monthly seasonality #

For the month seasonality we need twelve monthly averages. Again, we create a new column, now with months (January = 1, December = 12).

```
In [27]: df['month'] = df.index.month
    ...: df[['casual', 'member', 'month']].groupby('month').mean().round(1)
Out[27]: 
       casual  member
month                
1        74.4   107.4
2        88.4   129.2
3        94.3   133.2
4       116.5   150.2
5       123.9   152.8
6       146.0   167.6
7       142.2   165.4
8       143.0   183.0
9       150.4   189.4
10      134.0   202.7
11      100.0   176.0
12       68.0   130.9
```

We can plot just plot the total demand, since the seasonal patterns are quite similar for casulas and members.

```
In [28]: df[['total', 'month']].groupby('month').mean().plot(figsize=(8,5), 
    title='Figure 11. Monthly seasonality', color='black', linewidth=1, legend=False);
```

![](https://github.com/mikecinnamon/DataSci/blob/main/Figures/03e-11.png)

## Homework

1. How is the trend of the demand of electric and classic bikes? Are classic bikes lagging behind electric bikes?

2. Check that the information about the start and end stations is missing only for electric bikes. If you take this missingness as an indication of dockless bike share, you can examine how dockless share has evolved during these three years. Would you say that dockless share is trending up?

3. Which are the top-10 starting stations? Are they the same as the top-10 ending stations?

4. Are there stations with very low activity, so you can consider dropping them? Which threshold would you apply?
